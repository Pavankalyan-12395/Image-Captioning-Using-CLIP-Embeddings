**ğŸ“Œ Image Captioning Using CLIP Embeddings**

An AI/ML project that generates meaningful captions for images using the CLIP (Contrastive Languageâ€“Image Pretraining) model.


**ğŸ“– Overview**

This project uses CLIP embeddings to analyze image features and generate text captions.
The model links visual components with natural language representations to produce accurate descriptions.

**ğŸ¯ Objectives**

- Build an end-to-end image captioning pipeline

- Use CLIP embeddings for imageâ€“text understanding

- Improve caption accuracy using semantic similarity

**ğŸ›  Tools & Technologies**

- Python

- CLIP Model (OpenAI)

- PyTorch

- Jupyter Notebook

ğŸ“‚ Project Structure
Image-Captioning-CLIP/

â”‚â”€â”€ model.ipynb

â”‚â”€â”€ requirements.txt

â”‚â”€â”€ images/

â”‚â”€â”€ sample_outputs/

â”‚â”€â”€ README.md

**ğŸ§ª How It Works**

- Extract image embeddings using CLIP

- Map them to text embeddings

- Select best-fit caption using similarity scores

**ğŸš€ How to Run**

Install libraries:

1.pip install 
    
- r requirements.txt


2.Open model.ipynb

3.Run the notebook to generate captions

**ğŸ“¸ Sample Output**

<img width="1448" height="298" alt="Screenshot 2025-12-11 082548" src="https://github.com/user-attachments/assets/f9e1e96e-c169-4337-b081-1e9edf05bfd0" />

**ğŸ‘¨â€ğŸ’» Author**

Esambadi Pavan Kalyan

LinkedIn: https://www.linkedin.com/in/pavan-kalyan-esambadi
